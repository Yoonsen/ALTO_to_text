{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_alto(ndir):\n",
    "\n",
    "    \"\"\"Hent ut alle ordene i alto-filen, og legg til paragraf og sidenummer. Mappen ndir peker til mappen der tarfilene ligger\"\"\"\n",
    "\n",
    "   \n",
    "\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    import shutil\n",
    "\n",
    "   \n",
    "\n",
    "    # XML-filene ligger i mappen ndir, så gå gjennom med os.walk()\n",
    "\n",
    "    # Alle filene blir liggende i variabelen f\n",
    "\n",
    "    r,d,f = next(os.walk(ndir))\n",
    "\n",
    "   \n",
    "\n",
    "    # hent sidene i teksten og legg dem i variabelen pages\n",
    "\n",
    "    # skip metadatafilene - tekstene har sidenummer representert som 4-sifrede nummer, f.eks. 0014\n",
    "\n",
    "    pages = []\n",
    "\n",
    "   \n",
    "\n",
    "    for page in f:\n",
    "\n",
    "        pag = page.split('.xml')[0].split('_')[-1]\n",
    "\n",
    "        try:\n",
    "\n",
    "            int(pag)\n",
    "\n",
    "            pages.append((page, int(pag)))\n",
    "\n",
    "        except:\n",
    "\n",
    "            True\n",
    "\n",
    "       \n",
    "\n",
    "    # Gå gjennom side for side og hent ut teksten. Delte ord blir lagt i variabelen hyph,\n",
    "\n",
    "    # teksten i text. Alle ord får et sekvensnummer relativt til boka det står i, samtidig\n",
    "\n",
    "    # som alle avsnitt blir nummerert fortløpende\n",
    "\n",
    "   \n",
    "\n",
    "    para_num = 1\n",
    "\n",
    "    word_num = 1\n",
    "\n",
    " \n",
    "\n",
    "    text = []\n",
    "\n",
    "    hyph = []\n",
    "\n",
    " \n",
    "\n",
    "    hyp1 = \"\"\n",
    "\n",
    "    hyp2 = \"\"\n",
    "\n",
    "   \n",
    "\n",
    "    # sorter variabelen pages på sidenummer, andre ledd i tuplet\n",
    "\n",
    "    for page in sorted(pages, key=lambda x: x[1]):\n",
    "\n",
    "        page_file = os.path.join(r, page[0])\n",
    "\n",
    "        page_num = page[1]\n",
    "\n",
    "       \n",
    "\n",
    "        # parse XML-fila og få tak i rotelementet root\n",
    "\n",
    "        tree = ET.parse(page_file)\n",
    "\n",
    "        root = tree.getroot()\n",
    "\n",
    "       \n",
    "\n",
    "        # Gå gjennom XML-strukturen via TextBlock, som er avsnittselementet\n",
    "\n",
    "        for paragraph in root.findall(\".//TextBlock\"):\n",
    "\n",
    "           \n",
    "\n",
    "            # Finn alle ordene i avsnittet, som attributter til elementet String,\n",
    "\n",
    "            # og sjekk om det foreligger en orddeling -\n",
    "\n",
    "            # i så fall ligger hele ordet i attributtet SUBS_CONTENT, mens første ledd av orddelingen\n",
    "\n",
    "            # ligger i CONTENT. Om det ikke er noen orddeling ligger ordet i attributtet CONTENT.\n",
    "\n",
    "            # Burde fungere også med orddelinger over sideskift\n",
    "\n",
    "           \n",
    "\n",
    "            # Ordet lagres sammen med sekvensnummeret og sekvensnummeret for avsnittet står i,\n",
    "\n",
    "            # i tillegg til sidenummeret, som kan være greit for oppslag i bokhylla, i forbindelse\n",
    "\n",
    "            # med generering av konkordanser.\n",
    "\n",
    "           \n",
    "\n",
    "            for string in paragraph.findall(\".//String\"):\n",
    "\n",
    "                if 'SUBS_TYPE' in string.attrib:\n",
    "\n",
    "                    if string.attrib['SUBS_TYPE'] == \"HypPart1\":\n",
    "\n",
    "                        tokens = tok.tokenize(string.attrib['SUBS_CONTENT'])\n",
    "\n",
    "                        for token in tokens:\n",
    "\n",
    "                            text.append((token, word_num, para_num, page_num))\n",
    "\n",
    "                            word_num += 1\n",
    "\n",
    "                    elif string.attrib['SUBS_TYPE'] == \"HypPart2\":\n",
    "\n",
    "                        hyp2 = string.attrib['CONTENT']\n",
    "\n",
    "                        hyph.append((hyp1, hyp2))\n",
    "\n",
    "                else:\n",
    "\n",
    "                    tokens = tok.tokenize(string.attrib['CONTENT'])\n",
    "\n",
    "                    for token in tokens:\n",
    "\n",
    "                        text.append((token, word_num, para_num, page_num))\n",
    "\n",
    "                        word_num += 1\n",
    "\n",
    "            para_num += 1\n",
    "\n",
    "    # returner teksten som en sekvens av tupler, sammen med orddelingene, også som en sekvens av tupler\n",
    "\n",
    "    return text, hyph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
